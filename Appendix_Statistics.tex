\chapter{Appendix Statistics}

\section*{Concepts from Mathematical Statistics}The sampling distribution of an estimator is the probability distribution of the estimator under repeated sampling.  The standard error of a measurement is essentially the standard deviation of the process by which the measurement was generated.  When the underlying probability distribution of the generating process is known the standard error can be used to calculate confidence intervals.  Otherwise Chebyshev's inequality can be used. The standard error of a sample from a population is the standard deviation of the sampling distribution and may be estimated as $\frac{\sigma}{\sqrt{n}}$

Completeness is the ability of a statistical estimator to ensures that the parameters of the underlying probability distribution representing the model can be estimated from the statistic.

Consider a family of probability distributions for a random variable $X$ parameterized by a parameter $\theta$. A quantity $T(X)$ that depends on the random variable $X$ but not on the parameter $\theta$ is called a statistic.  A statistic that captures all of the information in X that is relevant to the estimation of $\theta$ is called a sufficient statistic. Since the conditional distribution of $X$ given $T(X)$ does not depend on $\theta$, neither does the conditional expected value of $g(X)$ given $T(X)$. The conditional expected value is itself a statistic and so is available for use in estimation. If $g(X)$ is an estimator of $\theta$, then typically the conditional expectation of $g(X)$ given $T(X)$ is a better estimator of $\theta$. The Rao-Blackwell theorem makes this precise.

\section*{L-Estimators \& M-Estimators}Order statistics of a sample ${X_i}$ can be used to estimate quantiles. An L statistic is a linear combination of an order statistic.  L statistics are used in Box plots.  An M-Estimator is the minima of sums of functions of the sample data.  Maximum Likelihood and Least Squares are examples of M-Estimators.

\section*{Testing for normality and other distributions } Powerful inference methods can be employed when data is generated by a Gaussian process. This section describes techniques for testing the normality of a sample and comparing two samples.  Kolmogorov-Smirnov test uses the fact that the empirical cumulative distribution function is normal in the limit. It is a non-parametric and distribution free test. Given the empirical distribution
 \[F_n(x) = \frac{1}{n} \sum\limits_{n}^{i=1} \biggl\{\begin{array}{c}1 :x_i\leq x \\0 : x_i>x\\ \end{array}\]
 , and a test CDF\[ F(x)\] the K-S test statistics are $D_n^+ = max(F_n(x)-F(x) ) $ and $D_n^- = min(F_n(x)-F(x) )$ The generality of this test comes at a loss in precision near the tails of a distribution.  The K-S statistics are more sensitive near points close to the median, and are only valid for continuous distributions.  The Kuipers test uses the statistic $D_n^+ + D_n^- $ and is useful for detecting changes in time series since the statistic is invariant in ???? transformation of the dependent variable $ F_n$.  The Anderson-Darling test is based on the K-S test and uses the specific distribution to specify the ????critical values??? of the test.  The chi-squared is based on the sample histogram and allows comparison against a discrete distribution, but has the potential drawback of being sensitive to how the histogram is binned and requires more samples to be valid.  The Shapiro-Wilk test uses the expected values of the order statistics of $F(x)$ to calculate the test statistic.  It is sensitive to data that are very close together, and numerical implementations may suffer from a loss of accuracy for large sample sizes.

 K-S [Chakravarti, Laha, and Roy, (1967). Handbook of Methods of Applied Statistics, Volume I, John Wiley and Sons, pp. 392-394].  Shapiro-Wilk [Shapiro, S. S. and Wilk, M. B. (1965). "An analysis of variance test for normality (complete samples)", Biometrika, 52, 3 and 4, pages 591-611.]


\section*{Regression Methods}Standard least squares regression consists in fitting a line through the data points (training points in learning theory) that minimizes the sum of square residuals.  The underlying assumption is that the data and the response can be modeled by a linear relationship.  In the event that the model accurately captures the functional dependence of the response generated by the data, and under the assumptions that the data is corrupted by Gaussian noise, precise statistical inferences can be made on the model parameters.  Modifications to this standard model include nonlinear mapping of the input data, local fitting, biased estimators, subset selection, coefficient shrinking, weighted least squares, and basis expansion transformations.

The multiple regression model in matrix notation can be expressed
as
(BBCREVISIT)

These equations hold for the univariate and the multivariate case.

$\textbf{Y} = \textbf{X}{\beta} + \textbf{e}$.

where $\textbf{e} =_d N(\mathbf{\mu},\mathbf{\Sigma})$

$E \textbf{Y} = \mathbf{\mu} X \mathbf{\beta}$

$var \textbf{Y} = \sigma^2 \textbf{I}$

Maximum Likelihood and Least Squares give same estimator;

$\widehat{\mathbf{\beta}}=(X^TX)^{-1} X^T Y$

$\hat{\sigma}^2 = frac{1}{n} || Y- \hat{\mu} ||^2$

Multivariate regression is the extension to $YM = Xb + e$

Here $Y, X, b$, and $e$ are as described for the multivariate regression model and $M$ is an $m x s$ matrix of coefficients defining linear transformation of the dependent variables. The normal equations are $X'Xb = X'YM$ and a solution for the normal equations is given by $b = (X'X)-X'YM$. Here the inverse of $X'X$ is a generalized inverse if $X'X$ contains
redundant columns.

\section*{Generalized Linear Models}Suppose we have $n$ observations of $k$ dimensional data denoted $\{x_i\}_{i=1}^{k}$ and for each observation we have a response $y_i$. We wish to fit the observations to the responses. Generalized Linear Regression is a modeling technique that allows for non normal distributions and models non-linear relationships in the training data. M-estimators are used to fit a generalized linear model Ref Huber (1964).
A linear model $ Y =\Lambda(X)=X\beta + \epsilon$ fits a linear relationship between the dependent variables $Y_i$ and the predictor variables $X_i$ \begin{equation}Y_i=\Lambda(X_i)=b_o + b \circ X_i.\end{equation}
A generalized linear model $Y= g(\Lambda(X) ) + \epsilon $ fits the data to $ Y = g (X \circ W)$. Fitting the model consists of minimizing the objective function $\sum\limits_{i=1}^{n} g(e_i)=\sum\limits_{i=1}^{n} g(y_i- x_i \beta)$
, where $e_i$ are the residuals $y_i-x_i \beta$. We see that for ordinary least squares $g(e_i)=e_i^2$, and the usual matrix equations fall out by differentiating with respect to $\beta$. Carrying this out for general $g$ \begin{equation} \sum\limits_{i=1}^{n} \frac{\partial  g(y_i-x_i \beta)}{\partial \beta}=0 \end{equation} gives the system of $k+1$ equations to solve for estimating the coefficients $b_i$.  If we set$\alpha(x)=\frac{g'(x)}{x}$ and calculate the derivative above, we have to solve \begin{equation} \sum\limits_{i=1}^{n} \omega(e_i) (y_i-x_i \beta) x_i = 0 \end{equation}. Which gives rise to a weighted least squares where the weights depend on the residuals - which depend on the coefficients - which depend on the weights.  This suggests an iterative algorithm; \begin{equation} \beta^\tau = ( X^{t} W^{(\tau-1)} X )^{-1} X^{t} W^{\tau-1} y \end{equation} where $W_{ij}^{(\tau-1)}=\alpha(e_{i}^{(\tau-1})$.  Several parameterizations are popular for the exponential family. The most general form of the distribution \[ p(x,\theta) = f(x,\theta)e^{g(x,\theta)} \in C^2(\dblr \otimes \dblr ) \otimes C^2(\dblr \otimes \dblr )\].  The estimators derived below assume that $f$ and $g$ are separable, \[ p(x,\theta) = f(x) h(\theta) e^{\alpha(x) \beta(\theta)} \in C^2(\dblr) \otimes C^2(\dblr ) \otimes C^2(\dblr ) \otimes C^2(\dblr ) \].  From \[ \int\limits_{x=-\infty}^{x=+\infty} p(x,\theta) dx \: =1\] we get \[ \fderiv{\theta}{p(x,\theta)} = 0 = \sderiv{\theta}{p(x,\theta)}d \] Since the parametrization we have chosen for the exponential family allows, in the sequel we drop the notation for dependent variable and denote the derivative with a prime. \[ \fderiv{\theta}{p(x,\theta)} = \fderiv{\theta}{f h e^{\alpha \beta}} = h' f e^{\alpha \beta} + f h \alpha \beta' e^{\alpha \beta} = \bigl( \frac{h'}{h} + \alpha \beta' \bigr) p(x,\theta) \] which gives \[\int \fderiv{\theta}{p(x,\theta)} dx = \int \bigl( \frac{h'}{h} + \alpha \beta' \bigr) p(x,\theta) dx= \frac{h'}{h} \int p(x,\theta) dx \:+\: \beta' \int \alpha(x) p(x,\theta) dx = \frac{h'}{h}+\beta' E[\alpha(x)] \] so that \[E[\alpha(x)]=-\frac{h'}{h \beta'}\]. Continuing along this vein,\begin{gather*} 0 =\int \sderiv{\theta}{p(x,\theta)} dx = \int \fderiv{\theta}{\bigl( \frac{h'}{h} + \alpha \beta' \bigr) p(x,\theta) } dx =\\  \int \bigr(\frac{h''}{h}-\frac{(h')^2}{h^2}+\alpha \beta'' \bigl ) p(x,\theta) + (\frac{h'}{h}+\alpha \beta') \fderiv{\theta}{p(x,\theta)} \: dx = \\ \int \bigr(\frac{h''}{h}-\frac{(h')^2}{h^2}+\alpha \beta'' \bigl ) p(x,\theta) + (\frac{h'}{h}+\alpha \beta')^2 p(x,\theta) \: dx = \\ \int \bigr(\frac{h''}{h}-\frac{(h')^2}{h^2}+\alpha \beta'' \bigl ) p(x,\theta) + (\frac{h'}{h}+\alpha \beta')^2 p(x,\theta) \: dx = \\ \int \bigr(\frac{h''}{h}-\frac{(h')^2}{h^2}+\alpha \beta'' \bigl ) p(x,\theta) + (\alpha \beta'- E[\alpha(x)]\beta')^2 p(x,\theta) \: dx \end{gather*}  Keeping in mind that  \[ Var[a x ]= E[ (ax-E(ax)^2 ] = a^2 E [ (x-E[x])^2] =a^2 Var[x]  \]  we get the variance via\[ \bigr(\frac{h''}{h}-\frac{(h')^2}{h^2}+E[\alpha(x)] \beta'' \bigl )+ Var[\alpha(x) \beta'(\theta)] = \bigr(\frac{h''}{h}-\frac{(h')^2}{h^2}+E[\alpha(x)] \beta'' \bigl )+ (\beta')^2 Var[\alpha(x)] = 0 \]  The score $U(x)$ is given by \[ U(x)=\pfderiv{\theta}{L(\theta,x)} = \pfderiv{\theta}{\log \: p(x,\theta)}=\pfderiv{\theta}{\bigr( \log h(\theta) + \log f(x) + \alpha(x) \beta(\theta)\bigl) } = \frac{h'}{h} + \alpha \beta'\] so \[E[U(x)]=\beta'E[\alpha(x)] + \frac{h'}{h} =0 \].  The Fisher Information $\mathcal{F}$ is defined \[\mathcal{F}=Var[U(x)] =Var[ \alpha \beta' + \frac{h'}{h}]= Var[ \alpha \beta'] \] So from above we have \[Var[U(x)]= Var[ \alpha \beta'] =\bigr(-\frac{h''}{h}+\frac{(h')^2}{h^2}-E[\alpha(x)] \beta'' \bigl )\].  Now differentiating,  \[ \fderiv{\theta}{U(\theta,x)} = \frac{h''}{h} - \frac{(h')^2}{h^2} + \alpha \beta''\] \[E[U'(\theta,x)]=\frac{h''}{h} - \frac{(h')^2}{h^2} + E[\alpha] \beta'' =  \frac{h''}{h} - \frac{(h')^2}{h^2} -\frac{ \beta'' h'}{\beta'}= - Var[U(x)] \].  Note that if we write the parametrization of the separable exponential family as \[ p(x,\theta) =  e^{\alpha(x) \beta(\theta)+\log(f(x))+\log(h(\theta))}\] then, \[\sderiv{\theta}{\log(h(\theta))}=\fderiv{\theta}{\frac{h'}{h}}=\frac{h''}{h}-\frac{(h')^2}{h^2}\].     (BBCREVISIT - this duplicates and has notation clash with material above).

A general form of the exponential distribution \begin{equation} \rho(x;\theta) = exp( \frac{x \theta - \xi(\theta) }{\sigma} ) \nu ( x) \end{equation} has a log likelihood for a random sample $\{ X_i  \}_{i=1 \hdots N}$ given functionally by \begin{equation} \mathcal{L} (\theta) =  \sum\limits_{i=1}^{N} [ X_i  \theta  - \xi (\theta) + log   ( \nu ( X_i ) ) ] \end{equation} The scale parameter $\sigma$ and $\theta$ are orthogonal parameters in that E [ ] The Generalized Linear model can  $\rho'$ is referred to as a link function in the statistical literature.  If $\rho'(x)= x\field(1)$ and $\epsilon=(\epsilon_1, \hdots ,\epsilon_n)$ are iid $N(\mu,\sigma)$ we have multiple linear regression.  In classification problems or binomial models the logit $\rho'(x)=log(x/(1-x))$ link function is used. The logit is extended to the $k$ category case by \begin{equation}\rho'( x_i | x_j j \neq i)= log ( \frac{x_i}{1- \sum\limits_{j \neq i} x_j})\end{equation}. The posterior probability densities ${p_i(?)}$ bbcrevisit (or $p_i$ the probability of observing class $i$)  of k classes are modeled by linear functions of the input variables $x_i$.

\section*{Fitting the GLM}Iteratively re-weighted least squares (IRLS) is used to for fitting generalized linear models and in finding M-estimators.  The objective function

\begin{equation}
J(\beta^{i+1}) = arg min \sum w_i ( \beta) | y_i - f_i (\beta) |
\end{equation}

is solved iteratively using a Gauss-Newton or Levenberg-Marquardt (LM) algorithm. LM is an iterative technique that finds a local minimum of a function that is expressed as the sum of squares of nonlinear functions. It is a combination of steepest descent and the Gauss-Newton method. When the current solution is far from the minimum the next iterate is in the direction of steepest descent. When the current solution is close to the minimum the next iterate is a Gauss-Newton step.

Linear least-squares estimates can behave badly when the error is not normal.  Outliers can be removed, or accounted for by employing a robust regression that is less sensitive to outliers than least squares.  M-Estimators introduced by Huber generalize maximum likelihood estimation and are less biased and more efficient.  Instead of trying to minimize the log likelihood

\begin{equation}
L(\theta) = \sum - log ( p(x_i, \theta)
\end{equation}

Huber proposed minimizing

\begin{equation}
M(\theta) = \sum  \rho(x_i, \theta)
\end{equation}

where $\rho$ reduces the effect of outliers. Common loss function are the Huber, and Tukey Bisquare.  For $\rho(x) = x^2$ we have the familiar least squares loss.

M estimators arise from the desire to apply Maximum Likelihood Estimators to noisy normal data, and to model more general distributions. They provide a regression that is robust against outliers in the training set, and allow for modeling of non-Gaussian processes. When $\rho$ above is a probability distribution, we are preforming a maximum likelihood estimation.

The Huber function which is a hybrid $L^2$ $L^1$ norm
\begin{equation}
\rho_\eta(e_i)=\biggl\{\begin{array}{cc}
\frac{e_i^2}{2} & |e_i| \leq \eta \\
  \eta |e_i| - \frac{\eta^2}{2} & |e_i| > \eta \\
\end{array}
\end{equation}
The  Tukey Bisquare estimator is given by
\begin{equation}
g_\eta(e_i)=\biggl\{ \begin{array}{cc} \frac{\eta^2}{6} (
1-[1-\frac{e_i}{\eta}_2]^3) & |e_i| \leq \eta \\
\frac{\eta^2}{6} & |e_i| > \eta \\
\end{array}
\end{equation}

Numerical procedures for doing this calculation are the Newton-Raphson method [see the section on root finding below ], and Fisher-Scoring method [ replace $ \frac{\partial^2 \mathcal{L}(\mathbf{\theta})}{\partial \mathbf{\theta} \partial \mathbf{\theta}^{t} }$ with $E[ \frac{\partial^2 \mathcal{L}(\mathbf{\theta})}{\partial \mathbf{\theta} \partial \mathbf{\theta}^{t} }  ]$. For high dimensional data, many models may be fit in an attempt to find the simplest one that can explain the data.  In the language of statistical learning theory, the choice of a norm $\rho$ is tantamount to choosing a loss function. Restricting the admissible functions to the one parameter family of exponential probability distributions defines the capacity via a functional form of the law of large numbers. \cite{Scholkopf B. (2002)}



\section*{Feature Subset Selection (FSS)}The goal of feature selection techniques to to improve the model building process by eliminating features that do not have discriminative power. Algorithms for feature selection either rank features or create subsets of increasing optimality.  FSS should be contrasted with feature extraction techniques such as PCA, LLE, or Laplacian eigenmaps.  The goal of feature extraction is to transform data from a high dimensional space to a low dimensional one while preserving the relevant information.

The statistical approach to feature selection most commonly used is stepwise regression.  Common optimality criteria are FSS schemes the Kolmogorov-Smirnov Test ,the t-test, the f-test, the Wilks Lambda Test and Wilcoxon Rank Sum Test.

It's important to distinguish the FSS process from a data dimension reduction process such as PCA which requires all the original measurements to compute the projection. The better FSS algorithms are recursive

Construct a $p x M$ basis matrix $H^{T}$ and transform feature vector $x' = H^{T} x$.


Generalize to $L^{2}$ with smoothing splines

Smoothing spline $RSS(f,\lambda)= \sum\limits_{i=1}^{N} (y_{i} -f(x_{i}) )^{2} + \lambda \int f''(t)^{2} dt$. where $f \in C^{2}(\field{R} )$ This is minimized in $L^{2}$ the first term measuring closeness of fit, and the second term penalizes curvature. $\lambda \rightarrow 0$ gives any function interpolating the data points ${x_i}_{i  \in {1, ... N} } $ an $\lambda \rightarrow \infty$ constrains $f$ to be linear.

\section*{Longitudinal Data Analysis} Longitudinal data analysis is the observation of multiple subjects over repeated intervals. Binary repeated responses are typically modelled with a marginal or random effects model, which will be made precise below. Marginal Models are a generalization of the GLM presented above for correlated data.  Here, the correlation is inter subject across time.  Statistical analysis of longitudinal data must take into account that serial observations of a subject are likely to be correlated, time may be an explanatory variable, and that missing response data my induce a bias in the results.  Let ${X_{ij}}$ be time varying or fixed covariates for the binary response ${Y_{ij}}$ of subject $i \in {1,...n}$ at time intervals $j \in {t_1,...t_m}$. By convention $X_{ij} \in \field{R} x \field{R^p}$ where the first dimension is the intercept. The marginal model is; $logit (E(Y_{ij} | X_{ij}) ) = X_{ij}^{\dagger} \beta$ and enforces the assumption that the relationship between the covariates and the response is the same for all subjects. Recall that for a binary response, $E(Y_{ij} | X_{ij}) = P(Y_{ij}=1 | X_{ij})$.  The random effects model takes into account that the relationship between the covariates and response varies between subjects; $logit (P(Y_{ij}=1 | X_{ij}) ) = X_{ij}^{\dagger} \beta_i$ If it is know that only a subset of the covariates are involved in the inter-subject variability, we can set $\beta_i= \beta + \beta_i$ and write $logit (P(Y_{ij}=1 | X_{ij}, \beta_i) ) = X_{ij}^{\dagger} \beta + O X_{ij} \beta_i$ Where the kernel of $O : \dblr^n \rightarrow \dblr^{n'}$ is the span of the covariates that do not change between subjects.  If $\lambda_i =_d N(0,\sigma)$ then the difference in the parameter vectors $\beta$ in the two models differ according to $\sigma$.

The GEE method of fitting the marginal model is described in: \cite{Liang, K-Y and Zeger, S. L.(1986)}  The Survival Analysis is a form of longitudinal analysis that takes into consideration the amount of time an observation is made on a subject.  GLM's can be used to fit discrete longitudinal hazard models derived from survival analysis, see  \cite{Prentice and Gloeckler (1978)}.   \cite{Meyer, B.D. (1990)} generalized that approach to account for an unobserved subject heterogeneity.  \cite{Holmen, M (2005)} applied the hazard model of \cite{Prentice, R. and L. Gloeckler (1978)} to the takeover hazard of large firms.  A negative relationship between dual class ownership and value is empirically known, and that relationship can be explained by the lower takeover probability of the dual class firms.  Dual class entities had a higher risk for takeover, but the hazard is lower since these firms use the dual class structure to change the capital structure in a way that allows the controlling shareholders to remain in control by reducing firm value.  The proportional hazards model can be discretized, but it is important to identify whether the process is truly a discrete process.  In that case the link function should be the logit as the Marginal Model above specifies, rather than the log-log function of the discretized proportional model.  The difference is the modelling of a probability transition in the former case versus a rate for the latter case.  Variable selection techniques for longitudinal data are relatively limited and most seem to rely on Wald type tests. Wald tests to include a variable are based on already computed maximum likelihood values. The Rao score test is used to include a covariate in the model building process.  The Wald test calculates \[z^2=\frac{\widehat{\beta}}{stderr}=_d  \chi^2\]  The likelihood ratio statistic for comparing two models $L_0 \in L_1$ \[-2 \frac{L_0}{L_1} =_d \chi^2\] is useful for backward stepwise variable subset selection. The degrees of freedom of the of the statistic is equal to the difference in dimension of the two models.


\section*{Discretization \& Sheppard's Correction}W. Sheppard (1898) Derived an approximate relationship between the moments of a continuous distribution and it's discrete approximation. This provides a transformation to statistical estimators that correct for the binning of continuous data.  As the scale at which datum are collected is increased, the variance of an estimate can become biased.  It is important to assess  bias caused by grouping and to correct it if necessary. The  bias of the approximate maximum likelihood estimator where observations are approximated by interval midpoints $O(w^2)$, where $w$ is the bin width. A Sheppards correction can be used to reduce the bias to order $O(w^3)$,  Signal processing engineers often have to deal with such a quantization effect when designing finite precision systems, image processing being a particularly relevant example. The engineering community typically models the quantization noise $Q=[X]-X$, where $[X]$ is the quantized realization of $X$. One might be tempted to apply a Sheppard's correction to the moments of the quantized data, thinking that $Var(X)<Var([X])$ but it is possible to construct examples where $Q$ and $[X]$ are independent, or where $Cov(X,Q)$ is such that $Var(X)>Var([X])$.  Shepard's correction is limited in that is doesn't apply to the first moment, and the frequencies of the first and last bins need to be low.  Expand $p(x;\theta)$ in a Taylor series and substitute in the Maximum Likelihood equations. \cite{Lindley, D. V. (1950)}  Suppose we have n realizations of iid RV's ${X_1, \hdots , X_n}$ and the data is collected on a discrete grid on the range of $X$ $Ran(X)=\{[y_i-d_i/2,y_i+d_i/2]\}_{i=1}^{i=m}$ where the intervals are centered on the location where a measurement. The realized values ${y_1, \hdots , y_m}$ have probabilities $p_i=\int\limits_{y_i - d_i /2}^{y_i+d_i /2} p(x;\theta) \;\; dx$ Expanding $p(x;\theta)$ in a Taylor series about $y$, $p(x;\theta)= \sum\limits_{i=0}^{\infty} \frac{p^{(i)}(y) }{i!} (x-y)^i$.


\section*{Multidimensional Scaling}Multidimensional scaling (MDS) is an alternative to factor analysis. The aim of MDS and factor analysis is to detect meaningful underlying dimensions that explain similarities or dissimilarities data points. In factor analysis, the similarities between points are expressed via the correlation matrix. With MDS any kind of similarity or dissimilarity matrix may be used.  Given $n$ observations ${x_i}_{i=1}^{n} \in \dblr^k$ and $n^2$ distances $d_{ij}$ between them, MDS looks for $n$ points ${\xi_i}_{i=1}^{n}$ in $dblr^l : l<k$ that preserve the distance relations. When a metric $\rho()$ exists for the similarity measure, gradient descent is used to minimize the MDS functional $S(\xi_1, \ldots , \xi_l)=\biggl( \sum_{i \neq j} d_{ij}-||\xi_i-\xi_j||_{\rho}\biggr)^\frac{1}{2}$.


\section*{Principal Components} For a data set $\textbf{X} \in
M_{(N,m)}(\mathbb{R}) = { x_1, x_2, \ldots x_N | x_i \in
\mathbb{R}^m } $, the first k principal components provided the
best k dimensional linear approximation to that data set.
Formally, we model the data via $f(\theta) = \mu + \textbf{V}_k
\theta | \mu \in \mathbb{R}^m, V_k \in O_{m,k}(\mathbb{R}),
\theta \in \mathbb{R}^k$ so $f(\theta)$ is an affine hyperplane
in $\mathbb{R}^m$


\section*{Evaluating classifier performance} Multi-class problems can be treated simultaneously or broken in to a sequence of two class problems.  Cross validation is used both for classifier parameter tuning and for feature subset selection.  Student-t and ANOVA can be used to evaluate the performance of classifiers against one another.  The Student-t test compares two classifiers, while the ANOVA test can compare multiple classifiers against one another.  Confusion matrices and ROC graphs are commonly employed visualization tools for assessing classifier performance. The rows of a confusion matrix add to the total population for each class, and the columns represent the predicted class.  An ROC curve plots the TP rate against the FP rate. Often a curve in ROC space is drawn using classifier parameters for tuning purposes.

\begin{table}[h]
\begin{tabular}{|c|c|}
  \hline
  % after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
  TN &  FP \\
  \hline
  FN & TP \\
  \hline
\end{tabular}
\caption{Two class confusion matrix where the proportions are
specified}
\end{table}

Common performance metrics for the two class problem are sensitivity (TP), specificity (TN), precision (the proportion of predicted cases within a class that were correct), and accuracy (the overall proportion of correct predictions). These metric can be extended to more than two classes by defining $A=tr ( C ) / || C ||_{L^\infty}$ where $C$ is the confusion matrix. TP, FN, FP, TN are proportions defined for the two class problem.


\section*{Covariance Matrix Estimation } For numerical stability in regression algorithms, the covariance matrix needs to be positive definite.  An well conditioned estimator for the covariance matrix of a process can be obtained by mixing the sample covariance with the identity matrix.  This is a linear shrinkage estimator based on a modified Frobenius norm for $A \in M_{mn}$  \begin{equation} ||\mathbf{A}||_{\cal{F}}= \sqrt{ \frac{tr (A A^t)}{n}} \end{equation}  Without loss of generality, set $\mu =0$ and let $\widehat{\Sigma} = \alpha \mathbb{I} + \beta \mathbf{S}$ where $\mbf{S}=\frac{\mbf{X}^T \mbf{X}}{n}$ is the sample covariance.  We seek to minimize $E( ||\widehat{\Sigma} - \Sigma||^2)$, but since we don't know the true population covariance matrix, we have to form an approximation.

Many applications in statistics and machine learning require an estimate of the covariance matrix or it's inverse. Generally we avoid taking inverses of matrices in practice for stability reasons. The sample covariance matrix usually performs poorly as a proxy for the underlying covariance matrix.  There is a large literature on this subject; particularly fomr the finance industry where this is an important part of portfolio theory.two major challenges in covariance estimation are the positive-defniteness constraint and the high-dimensionality where the number of parameters grows quadratically in the dimension.

\section*{Testing For Normality}In this section we will use the term $EPDF_X$ to mean the empirical probability density function. There are a variety of univariate tests to help determine which parametric distribution your data belongs to. These fall under the category of Goodness of Fit testing. For a parametric family the null hypothesis $H_o : X=_d p(x| \theta)$ is tested against the alternative that $X$ does not belong to the family $p(x|\theta)$ There are also family of test to determine whether two $EPDF$'s come from the same distribution.  Keep in mind that there are many transformations ( polynomial, logarithmic, \& rational )  to transform data to look more Normal.  The presence of tails and skew will be the most problematic to deal with.

The Anderson-Darling test determines whether a sample comes from a specified distribution. The sample data can is transformed to a uniform distribution and then a uniformity test is then done on the transformed data. The test statistic is compared against pre-computed values for the assumed probability distribution.

The Kolmogorov-Smirnov is non-parametric a form of minimum distance estimation.  It can be used to test a sample against a reference or to compare two samples against each other.  In the one sided case the KS statistic calculates the distance between the $EPDF$ of a sample and a reference.  In the two sided case the distance between the $EPDS$'f of the two samples are calculated.  The KS test is robust to location and shape, making it  Omnibus tests evaluate whether the explained variance in a set of data is significantly greater than the unexplained variance. For example is the F-test in ANOVA. Omnibus tests of normality based on the likelihood ratio outperform the Anderson-Darling test statistic.

